{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDSPGmMhUYmks1I9B5d2fo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e9acdb1a1d2643819339b8f0cf3cecd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_135dbaaa41a946ca899dc8164bca3e44",
              "IPY_MODEL_09db8fe0b55443fdab2ac54e84bb7ff5",
              "IPY_MODEL_b591e297adff46fa8df014fe9192caa3"
            ],
            "layout": "IPY_MODEL_8c2682025ce14a88b39dbfc826e33954"
          }
        },
        "135dbaaa41a946ca899dc8164bca3e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ad3705618644447bfcfdb9b65271883",
            "placeholder": "​",
            "style": "IPY_MODEL_b71e32972c104b53a669ec7591e557ff",
            "value": "100%"
          }
        },
        "09db8fe0b55443fdab2ac54e84bb7ff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_861ab3f25cb4479f9ace3978dc315ecf",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52eabebd19834610890b10347acaf05a",
            "value": 3
          }
        },
        "b591e297adff46fa8df014fe9192caa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e39bf65dc61a4836805a0a8bd23503a3",
            "placeholder": "​",
            "style": "IPY_MODEL_a864568c3b89405cb3ca8e36180f9e61",
            "value": " 3/3 [00:00&lt;00:00, 66.64it/s]"
          }
        },
        "8c2682025ce14a88b39dbfc826e33954": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ad3705618644447bfcfdb9b65271883": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b71e32972c104b53a669ec7591e557ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "861ab3f25cb4479f9ace3978dc315ecf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52eabebd19834610890b10347acaf05a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e39bf65dc61a4836805a0a8bd23503a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a864568c3b89405cb3ca8e36180f9e61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e2728489569432394255a3bb3a80247": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_af6bdd67d22a42f0be80845511720ae0",
              "IPY_MODEL_3b32dbed334941b881ae0e563d7c40ce",
              "IPY_MODEL_6abc6629a8b0412c9b09ad02c3376127"
            ],
            "layout": "IPY_MODEL_8722d84268664df5a8012a83afd575f3"
          }
        },
        "af6bdd67d22a42f0be80845511720ae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ddd98a8524042adba2bbaab1dbf60af",
            "placeholder": "​",
            "style": "IPY_MODEL_8a1e9c4517dc463ba78b91b16431008a",
            "value": ""
          }
        },
        "3b32dbed334941b881ae0e563d7c40ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a8902cb735d4f658fd387204149ace6",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e25584843ddd4a4484ec94e7e6c0f6a0",
            "value": 0
          }
        },
        "6abc6629a8b0412c9b09ad02c3376127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_155517c19bc24d599b068af7e28ad8cf",
            "placeholder": "​",
            "style": "IPY_MODEL_3838ac5a07244d7da893c8ba71b21fd9",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "8722d84268664df5a8012a83afd575f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ddd98a8524042adba2bbaab1dbf60af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a1e9c4517dc463ba78b91b16431008a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a8902cb735d4f658fd387204149ace6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "e25584843ddd4a4484ec94e7e6c0f6a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "155517c19bc24d599b068af7e28ad8cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3838ac5a07244d7da893c8ba71b21fd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea941579121247548fdf7774079f2f5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0faf64a0c2af4340ac3b0eed24732d5a",
              "IPY_MODEL_cde2d6e48a7748ca840c745e52fa06c2",
              "IPY_MODEL_7874b0b368c24ef7b15fac3a4ed83582"
            ],
            "layout": "IPY_MODEL_a255def76d9b4c9db5d9cc7385fc9e6d"
          }
        },
        "0faf64a0c2af4340ac3b0eed24732d5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0614b4ade314c5c8073193bb1d3408f",
            "placeholder": "​",
            "style": "IPY_MODEL_38b21b8459234a42b8fb69752644d48a",
            "value": "100%"
          }
        },
        "cde2d6e48a7748ca840c745e52fa06c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ffb8a1db11a43cb8602ec7af8c55f77",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5293a6fc609849eda02622c6fa5b2167",
            "value": 1
          }
        },
        "7874b0b368c24ef7b15fac3a4ed83582": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9403580e4524754980170c5f1130bd9",
            "placeholder": "​",
            "style": "IPY_MODEL_1e5da5a29aaa4700bcdbdfc768b33aaf",
            "value": " 1/1 [00:00&lt;00:00,  8.90ba/s]"
          }
        },
        "a255def76d9b4c9db5d9cc7385fc9e6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0614b4ade314c5c8073193bb1d3408f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38b21b8459234a42b8fb69752644d48a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ffb8a1db11a43cb8602ec7af8c55f77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5293a6fc609849eda02622c6fa5b2167": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c9403580e4524754980170c5f1130bd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e5da5a29aaa4700bcdbdfc768b33aaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azizbarank/distilroberta-base-sst-2-distilled/blob/main/knowledge_distillation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing necessary packages"
      ],
      "metadata": {
        "id": "xknRaAfngCT1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "285QJOlK9Z2W",
        "outputId": "c4169aea-40e4-4e75-8c29-d3faa58ff92b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.22.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.5.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (5.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.8.2)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.2.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.49.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.4.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.17.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.35.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (2.3.4-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 12 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets tensorboard\n",
        "!sudo apt-get install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chhosing our \"teacher\" and \"student\" models"
      ],
      "metadata": {
        "id": "TFefM0LdgJkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "student = \"distilroberta-base\"\n",
        "teacher = \"textattack/roberta-base-SST-2\""
      ],
      "metadata": {
        "id": "wrQyPjcl9-Ol"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading our SST-2 part of the GLUE dataset"
      ],
      "metadata": {
        "id": "Mxo4XGgRgQhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"glue\",\"sst2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "e9acdb1a1d2643819339b8f0cf3cecd1",
            "135dbaaa41a946ca899dc8164bca3e44",
            "09db8fe0b55443fdab2ac54e84bb7ff5",
            "b591e297adff46fa8df014fe9192caa3",
            "8c2682025ce14a88b39dbfc826e33954",
            "5ad3705618644447bfcfdb9b65271883",
            "b71e32972c104b53a669ec7591e557ff",
            "861ab3f25cb4479f9ace3978dc315ecf",
            "52eabebd19834610890b10347acaf05a",
            "e39bf65dc61a4836805a0a8bd23503a3",
            "a864568c3b89405cb3ca8e36180f9e61"
          ]
        },
        "id": "ZhFP95R3-xs-",
        "outputId": "f5248a08-07c1-4640-f397-a370c2a60e53"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9acdb1a1d2643819339b8f0cf3cecd1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "nM5nz3GOgXoG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initiating the tokenizer of our student model"
      ],
      "metadata": {
        "id": "mlAARAR8ge9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(student)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "9e2728489569432394255a3bb3a80247",
            "af6bdd67d22a42f0be80845511720ae0",
            "3b32dbed334941b881ae0e563d7c40ce",
            "6abc6629a8b0412c9b09ad02c3376127",
            "8722d84268664df5a8012a83afd575f3",
            "3ddd98a8524042adba2bbaab1dbf60af",
            "8a1e9c4517dc463ba78b91b16431008a",
            "0a8902cb735d4f658fd387204149ace6",
            "e25584843ddd4a4484ec94e7e6c0f6a0",
            "155517c19bc24d599b068af7e28ad8cf",
            "3838ac5a07244d7da893c8ba71b21fd9"
          ]
        },
        "id": "-pTJHVMv-7fz",
        "outputId": "d5047064-b098-435b-938c-ffd0c4a3ed9b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moving 0 files to the new cache system\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e2728489569432394255a3bb3a80247"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"sentence\"], truncation=True, max_length=512\n",
        "    )\n",
        "    return tokenized_inputs\n",
        "\n",
        "sst2_enc = dataset.map(process, batched=True)\n",
        "sst2_enc = sst2_enc.rename_column(\"label\",\"labels\")\n",
        "\n",
        "sst2_enc[\"test\"].features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "ea941579121247548fdf7774079f2f5b",
            "0faf64a0c2af4340ac3b0eed24732d5a",
            "cde2d6e48a7748ca840c745e52fa06c2",
            "7874b0b368c24ef7b15fac3a4ed83582",
            "a255def76d9b4c9db5d9cc7385fc9e6d",
            "b0614b4ade314c5c8073193bb1d3408f",
            "38b21b8459234a42b8fb69752644d48a",
            "5ffb8a1db11a43cb8602ec7af8c55f77",
            "5293a6fc609849eda02622c6fa5b2167",
            "c9403580e4524754980170c5f1130bd9",
            "1e5da5a29aaa4700bcdbdfc768b33aaf"
          ]
        },
        "id": "4W4oB3mzHJlb",
        "outputId": "60e0f718-85e2-4b4b-ae18-48e52f6453cb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-eb8b81f55b807494.arrow\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea941579121247548fdf7774079f2f5b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-59191ea9c39cabe0.arrow\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentence': Value(dtype='string', id=None),\n",
              " 'labels': ClassLabel(num_classes=2, names=['negative', 'positive'], id=None),\n",
              " 'idx': Value(dtype='int32', id=None),\n",
              " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
              " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating our Knowledge Distillation Trainer"
      ],
      "metadata": {
        "id": "uGE72or9gsGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "class DistillationTrainingArguments(TrainingArguments):\n",
        "    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature"
      ],
      "metadata": {
        "id": "JayX6RkQ_GZf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import Trainer\n",
        "\n",
        "class DistillationTrainer(Trainer):\n",
        "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.teacher = teacher_model\n",
        "        self._move_model_to_device(self.teacher,self.model.device)\n",
        "        self.teacher.eval()\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "\n",
        "        # compute student output\n",
        "        outputs_student = model(**inputs)\n",
        "        student_loss=outputs_student.loss\n",
        "        # compute teacher output\n",
        "        with torch.no_grad():\n",
        "          outputs_teacher = self.teacher(**inputs)\n",
        "\n",
        "        # assert size\n",
        "        assert outputs_student.logits.size() == outputs_teacher.logits.size()\n",
        "\n",
        "        # compute distillation loss and soften probabilities\n",
        "        loss_function = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "        loss_logits = (loss_function(\n",
        "            F.log_softmax(outputs_student.logits / self.args.temperature, dim=-1),\n",
        "            F.softmax(outputs_teacher.logits / self.args.temperature, dim=-1)) * (self.args.temperature ** 2))\n",
        "        # return weighted student loss\n",
        "        loss = self.args.alpha * student_loss + (1. - self.args.alpha) * loss_logits\n",
        "        return (loss, outputs_student) if return_outputs else loss"
      ],
      "metadata": {
        "id": "GdeAjnlUERab"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Metric"
      ],
      "metadata": {
        "id": "Of9nToqqhF9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "\n",
        "accuracy_metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    acc = accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "    return {\n",
        "        \"accuracy\": acc[\"accuracy\"],\n",
        "    }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZl-LLhKJLIZ",
        "outputId": "e6d2270a-02d1-4074-f546-b536697d30fe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Training Arguments"
      ],
      "metadata": {
        "id": "zouXYIx0hnd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding\n",
        "from huggingface_hub import HfFolder\n",
        "\n",
        "# id2label, label2id dicts for the outputs for the model\n",
        "labels = sst2_enc[\"train\"].features[\"labels\"].names\n",
        "num_labels = len(labels)\n",
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = str(i)\n",
        "    id2label[str(i)] = label\n",
        "\n",
        "# training arguments\n",
        "training_args = DistillationTrainingArguments(\n",
        "    output_dir=\"distilroberta-base-sst2-distilled\",\n",
        "    num_train_epochs=7, per_device_train_batch_size=128,\n",
        "    per_device_eval_batch_size=128, fp16=True, \n",
        "    learning_rate=6e-5, seed=33, \n",
        "    logging_dir=f\"distilroberta-base-sst2-distilled/logs\",\n",
        "    logging_strategy=\"epoch\", evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\", save_total_limit=2, \n",
        "    load_best_model_at_end=True, metric_for_best_model=\"accuracy\", \n",
        "    report_to=\"tensorboard\", push_to_hub=False,\n",
        "    alpha=0.5, temperature=4.0\n",
        "    )\n",
        "\n",
        "# data_collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# teacher model\n",
        "teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    teacher,\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "\n",
        "# student model\n",
        "student_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    student,\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv-Et_22JOgk",
        "outputId": "c9d89407-8216-4807-dcc0-5afa607f6a04"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at textattack/roberta-base-SST-2 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "UWpFJ4gKiEri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = DistillationTrainer(\n",
        "    student_model,\n",
        "    training_args,\n",
        "    teacher_model=teacher_model,\n",
        "    train_dataset=sst2_enc[\"train\"],\n",
        "    eval_dataset=sst2_enc[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C94kC2PqLaP_",
        "outputId": "521b7405-56b7-4d3d-eac5-3e7d74cc0242"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cuda_amp half precision backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GLvdvmOFLeuG",
        "outputId": "a7894862-1460-4d79-e24b-a635e2f1ee8d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 67349\n",
            "  Num Epochs = 7\n",
            "  Instantaneous batch size per device = 128\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 3689\n",
            "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3689' max='3689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3689/3689 17:55, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.513400</td>\n",
              "      <td>0.435302</td>\n",
              "      <td>0.895642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.215300</td>\n",
              "      <td>0.433695</td>\n",
              "      <td>0.909404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>0.335221</td>\n",
              "      <td>0.917431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.117900</td>\n",
              "      <td>0.362863</td>\n",
              "      <td>0.917431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.095600</td>\n",
              "      <td>0.366062</td>\n",
              "      <td>0.915138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.081700</td>\n",
              "      <td>0.327687</td>\n",
              "      <td>0.916284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.072900</td>\n",
              "      <td>0.315920</td>\n",
              "      <td>0.920872</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 872\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to distilroberta-base-sst2-distilled/checkpoint-527\n",
            "Configuration saved in distilroberta-base-sst2-distilled/checkpoint-527/config.json\n",
            "Model weights saved in distilroberta-base-sst2-distilled/checkpoint-527/pytorch_model.bin\n",
            "tokenizer config file saved in distilroberta-base-sst2-distilled/checkpoint-527/tokenizer_config.json\n",
            "Special tokens file saved in distilroberta-base-sst2-distilled/checkpoint-527/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 872\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to distilroberta-base-sst2-distilled/checkpoint-1054\n",
            "Configuration saved in distilroberta-base-sst2-distilled/checkpoint-1054/config.json\n",
            "Model weights saved in distilroberta-base-sst2-distilled/checkpoint-1054/pytorch_model.bin\n",
            "tokenizer config file saved in distilroberta-base-sst2-distilled/checkpoint-1054/tokenizer_config.json\n",
            "Special tokens file saved in distilroberta-base-sst2-distilled/checkpoint-1054/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 872\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to distilroberta-base-sst2-distilled/checkpoint-1581\n",
            "Configuration saved in distilroberta-base-sst2-distilled/checkpoint-1581/config.json\n",
            "Model weights saved in distilroberta-base-sst2-distilled/checkpoint-1581/pytorch_model.bin\n",
            "tokenizer config file saved in distilroberta-base-sst2-distilled/checkpoint-1581/tokenizer_config.json\n",
            "Special tokens file saved in distilroberta-base-sst2-distilled/checkpoint-1581/special_tokens_map.json\n",
            "Deleting older checkpoint [distilroberta-base-sst2-distilled/checkpoint-527] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 872\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to distilroberta-base-sst2-distilled/checkpoint-2108\n",
            "Configuration saved in distilroberta-base-sst2-distilled/checkpoint-2108/config.json\n",
            "Model weights saved in distilroberta-base-sst2-distilled/checkpoint-2108/pytorch_model.bin\n",
            "tokenizer config file saved in distilroberta-base-sst2-distilled/checkpoint-2108/tokenizer_config.json\n",
            "Special tokens file saved in distilroberta-base-sst2-distilled/checkpoint-2108/special_tokens_map.json\n",
            "Deleting older checkpoint [distilroberta-base-sst2-distilled/checkpoint-1054] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 872\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to distilroberta-base-sst2-distilled/checkpoint-2635\n",
            "Configuration saved in distilroberta-base-sst2-distilled/checkpoint-2635/config.json\n",
            "Model weights saved in distilroberta-base-sst2-distilled/checkpoint-2635/pytorch_model.bin\n",
            "tokenizer config file saved in distilroberta-base-sst2-distilled/checkpoint-2635/tokenizer_config.json\n",
            "Special tokens file saved in distilroberta-base-sst2-distilled/checkpoint-2635/special_tokens_map.json\n",
            "Deleting older checkpoint [distilroberta-base-sst2-distilled/checkpoint-2108] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 872\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to distilroberta-base-sst2-distilled/checkpoint-3162\n",
            "Configuration saved in distilroberta-base-sst2-distilled/checkpoint-3162/config.json\n",
            "Model weights saved in distilroberta-base-sst2-distilled/checkpoint-3162/pytorch_model.bin\n",
            "tokenizer config file saved in distilroberta-base-sst2-distilled/checkpoint-3162/tokenizer_config.json\n",
            "Special tokens file saved in distilroberta-base-sst2-distilled/checkpoint-3162/special_tokens_map.json\n",
            "Deleting older checkpoint [distilroberta-base-sst2-distilled/checkpoint-2635] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 872\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to distilroberta-base-sst2-distilled/checkpoint-3689\n",
            "Configuration saved in distilroberta-base-sst2-distilled/checkpoint-3689/config.json\n",
            "Model weights saved in distilroberta-base-sst2-distilled/checkpoint-3689/pytorch_model.bin\n",
            "tokenizer config file saved in distilroberta-base-sst2-distilled/checkpoint-3689/tokenizer_config.json\n",
            "Special tokens file saved in distilroberta-base-sst2-distilled/checkpoint-3689/special_tokens_map.json\n",
            "Deleting older checkpoint [distilroberta-base-sst2-distilled/checkpoint-1581] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from distilroberta-base-sst2-distilled/checkpoint-3689 (score: 0.9208715596330275).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3689, training_loss=0.17812904881151365, metrics={'train_runtime': 1077.7269, 'train_samples_per_second': 437.442, 'train_steps_per_second': 3.423, 'total_flos': 5988547867083024.0, 'train_loss': 0.17812904881151365, 'epoch': 7.0})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Optuna for Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "erF--_J-iQYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_uCIShyLkrD",
        "outputId": "fb9f8ee2-c618-4d47-cfab-7b7d8ab66539"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.0.2-py3-none-any.whl (348 kB)\n",
            "\u001b[K     |████████████████████████████████| 348 kB 41.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy<1.9.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.3)\n",
            "Collecting alembic>=1.5.0\n",
            "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 64.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 10.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.41)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from alembic>=1.5.0->optuna) (5.0.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic>=1.5.0->optuna) (5.9.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.3-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.3.0->optuna) (1.1.3)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.10.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 78.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.4.1)\n",
            "Collecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
            "Collecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 77.9 MB/s \n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (4.1.1)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (22.1.0)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->alembic>=1.5.0->optuna) (3.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=76d500f08ba8502ece4f7c398d6cb2efc12bbae86fffbada360142f27ddd9b87\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.2.3 alembic-1.8.1 autopage-0.5.1 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.2 colorlog-6.7.0 optuna-3.0.2 pbr-5.10.0 pyperclip-1.8.2 stevedore-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Hyperparamater Space to be optimized over"
      ],
      "metadata": {
        "id": "fFqHvgR5iWZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hp_space(trial):\n",
        "    return {\n",
        "      \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 2, 10),\n",
        "      \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-3 ,log=True),\n",
        "      \"alpha\": trial.suggest_float(\"alpha\", 0, 1),\n",
        "      \"temperature\": trial.suggest_int(\"temperature\", 2, 30),\n",
        "      }"
      ],
      "metadata": {
        "id": "pwerkX6TL-Z3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the Hyperparameter Search"
      ],
      "metadata": {
        "id": "a936pRODi43P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def student_init():\n",
        "    return AutoModelForSequenceClassification.from_pretrained(\n",
        "        student,\n",
        "        num_labels=num_labels,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    )\n",
        "\n",
        "trainer = DistillationTrainer(\n",
        "    model_init=student_init,\n",
        "    args=training_args,\n",
        "    teacher_model=teacher_model,\n",
        "    train_dataset=sst2_enc[\"train\"],\n",
        "    eval_dataset=sst2_enc[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "best_run = trainer.hyperparameter_search(\n",
        "    n_trials=2,\n",
        "    direction=\"maximize\",\n",
        "    hp_space=hp_space\n",
        ")\n",
        "\n",
        "print(best_run)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JBi0KiDuMBXP",
        "outputId": "febe0f20-83c0-45da-9420-03101e1a8c58"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/c1149320821601524a8d373726ed95bbd2bc0dc2/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"negative\",\n",
            "    \"1\": \"positive\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"negative\": \"0\",\n",
            "    \"positive\": \"1\"\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.22.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/c1149320821601524a8d373726ed95bbd2bc0dc2/pytorch_model.bin\n",
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using cuda_amp half precision backend\n",
            "\u001b[32m[I 2022-10-08 06:42:14,725]\u001b[0m A new study created in memory with name: no-name-b5a8189a-9935-442f-b7b7-3d1a77df9d9f\u001b[0m\n",
            "Trial:\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/c1149320821601524a8d373726ed95bbd2bc0dc2/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"negative\",\n",
            "    \"1\": \"positive\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"negative\": \"0\",\n",
            "    \"positive\": \"1\"\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.22.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/c1149320821601524a8d373726ed95bbd2bc0dc2/pytorch_model.bin\n",
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 67349\n",
            "  Num Epochs = 9\n",
            "  Instantaneous batch size per device = 128\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 4743\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4743' max='4743' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4743/4743 23:09, Epoch 9/9]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.590600</td>\n",
              "      <td>0.479539</td>\n",
              "      <td>0.895642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.247800</td>\n",
              "      <td>0.480304</td>\n",
              "      <td>0.909404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.173100</td>\n",
              "      <td>0.476644</td>\n",
              "      <td>0.909404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.132000</td>\n",
              "      <td>0.438816</td>\n",
              "      <td>0.909404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.102800</td>\n",
              "      <td>0.452740</td>\n",
              "      <td>0.907110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.084300</td>\n",
              "      <td>0.392715</td>\n",
              "      <td>0.919725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.072300</td>\n",
              "      <td>0.381166</td>\n",
              "      <td>0.918578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.061800</td>\n",
              "      <td>0.359256</td>\n",
              "      <td>0.917431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.055800</td>\n",
              "      <td>0.361851</td>\n",
              "      <td>0.922018</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 872\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to distilroberta-base-sst2-distilled/run-0/checkpoint-527\n",
            "Configuration saved in distilroberta-base-sst2-distilled/run-0/checkpoint-527/config.json\n",
            "Model weights saved in distilroberta-base-sst2-distilled/run-0/checkpoint-527/pytorch_model.bin\n",
            "tokenizer config file saved in distilroberta-base-sst2-distilled/run-0/checkpoint-527/tokenizer_config.json\n",
            "Special tokens file saved in distilroberta-base-sst2-distilled/run-0/checkpoint-527/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 872\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to distilroberta-base-sst2-distilled/run-0/checkpoint-1054\n",
            "Configuration saved in distilroberta-base-sst2-distilled/run-0/checkpoint-1054/config.json\n",
            "Model weights saved in distilroberta-base-sst2-distilled/run-0/checkpoint-1054/pytorch_model.bin\n",
            "tokenizer config file saved in distilroberta-base-sst2-distilled/run-0/checkpoint-1054/tokenizer_config.json\n",
            "Special tokens file saved in distilroberta-base-sst2-distilled/run-0/checkpoint-1054/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 872\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to distilroberta-base-sst2-distilled/run-0/checkpoint-1581\n",
            "Configuration saved in distilroberta-base-sst2-distilled/run-0/checkpoint-1581/config.json\n",
            "Model weights saved in distilroberta-base-sst2-distilled/run-0/checkpoint-1581/pytorch_model.bin\n",
            "tokenizer config file saved in distilroberta-base-sst2-distilled/run-0/checkpoint-1581/tokenizer_config.json\n",
            "Special tokens file saved in distilroberta-base-sst2-distilled/run-0/checkpoint-1581/special_tokens_map.json\n",
            "Deleting older checkpoint [distilroberta-base-sst2-distilled/run-0/checkpoint-527] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 872\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to distilroberta-base-sst2-distilled/run-0/checkpoint-2108\n",
            "Configuration saved in distilroberta-base-sst2-distilled/run-0/checkpoint-2108/config.json\n",
            "Model weights saved in distilroberta-base-sst2-distilled/run-0/checkpoint-2108/pytorch_model.bin\n",
            "tokenizer config file saved in distilroberta-base-sst2-distilled/run-0/checkpoint-2108/tokenizer_config.json\n",
            "Special tokens file saved in distilroberta-base-sst2-distilled/run-0/checkpoint-2108/special_tokens_map.json\n",
            "Deleting older checkpoint [distilroberta-base-sst2-distilled/run-0/checkpoint-1581] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 872\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to distilroberta-base-sst2-distilled/run-0/checkpoint-2635\n",
            "Configuration saved in distilroberta-base-sst2-distilled/run-0/checkpoint-2635/config.json\n",
            "Model weights saved in distilroberta-base-sst2-distilled/run-0/checkpoint-2635/pytorch_model.bin\n",
            "tokenizer config file saved in distilroberta-base-sst2-distilled/run-0/checkpoint-2635/tokenizer_config.json\n",
            "Special tokens file saved in distilroberta-base-sst2-distilled/run-0/checkpoint-2635/special_tokens_map.json\n",
            "Deleting older checkpoint [distilroberta-base-sst2-distilled/run-0/checkpoint-2108] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 872\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to distilroberta-base-sst2-distilled/run-0/checkpoint-3162\n",
            "Configuration saved in distilroberta-base-sst2-distilled/run-0/checkpoint-3162/config.json\n",
            "Model weights saved in distilroberta-base-sst2-distilled/run-0/checkpoint-3162/pytorch_model.bin\n",
            "tokenizer config file saved in distilroberta-base-sst2-distilled/run-0/checkpoint-3162/tokenizer_config.json\n",
            "Special tokens file saved in distilroberta-base-sst2-distilled/run-0/checkpoint-3162/special_tokens_map.json\n",
            "Deleting older checkpoint [distilroberta-base-sst2-distilled/run-0/checkpoint-1054] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 872\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to distilroberta-base-sst2-distilled/run-0/checkpoint-3689\n",
            "Configuration saved in distilroberta-base-sst2-distilled/run-0/checkpoint-3689/config.json\n",
            "Model weights saved in distilroberta-base-sst2-distilled/run-0/checkpoint-3689/pytorch_model.bin\n",
            "tokenizer config file saved in distilroberta-base-sst2-distilled/run-0/checkpoint-3689/tokenizer_config.json\n",
            "Special tokens file saved in distilroberta-base-sst2-distilled/run-0/checkpoint-3689/special_tokens_map.json\n",
            "Deleting older checkpoint [distilroberta-base-sst2-distilled/run-0/checkpoint-2635] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 872\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to distilroberta-base-sst2-distilled/run-0/checkpoint-4216\n",
            "Configuration saved in distilroberta-base-sst2-distilled/run-0/checkpoint-4216/config.json\n",
            "Model weights saved in distilroberta-base-sst2-distilled/run-0/checkpoint-4216/pytorch_model.bin\n",
            "tokenizer config file saved in distilroberta-base-sst2-distilled/run-0/checkpoint-4216/tokenizer_config.json\n",
            "Special tokens file saved in distilroberta-base-sst2-distilled/run-0/checkpoint-4216/special_tokens_map.json\n",
            "Deleting older checkpoint [distilroberta-base-sst2-distilled/run-0/checkpoint-3689] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 872\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to distilroberta-base-sst2-distilled/run-0/checkpoint-4743\n",
            "Configuration saved in distilroberta-base-sst2-distilled/run-0/checkpoint-4743/config.json\n",
            "Model weights saved in distilroberta-base-sst2-distilled/run-0/checkpoint-4743/pytorch_model.bin\n",
            "tokenizer config file saved in distilroberta-base-sst2-distilled/run-0/checkpoint-4743/tokenizer_config.json\n",
            "Special tokens file saved in distilroberta-base-sst2-distilled/run-0/checkpoint-4743/special_tokens_map.json\n",
            "Deleting older checkpoint [distilroberta-base-sst2-distilled/run-0/checkpoint-3162] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from distilroberta-base-sst2-distilled/run-0/checkpoint-4743 (score: 0.9220183486238532).\n",
            "\u001b[32m[I 2022-10-08 07:05:25,982]\u001b[0m Trial 0 finished with value: 0.9220183486238532 and parameters: {'num_train_epochs': 9, 'learning_rate': 0.00013970706124368524, 'alpha': 0.4213121676768802, 'temperature': 4}. Best is trial 0 with value: 0.9220183486238532.\u001b[0m\n",
            "Trial:\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/c1149320821601524a8d373726ed95bbd2bc0dc2/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"negative\",\n",
            "    \"1\": \"positive\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"negative\": \"0\",\n",
            "    \"positive\": \"1\"\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.22.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/c1149320821601524a8d373726ed95bbd2bc0dc2/pytorch_model.bin\n",
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 67349\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 128\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2108\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2108' max='2108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2108/2108 10:18, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.395800</td>\n",
              "      <td>0.349726</td>\n",
              "      <td>0.904817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.184200</td>\n",
              "      <td>0.385815</td>\n",
              "      <td>0.916284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.117900</td>\n",
              "      <td>0.348604</td>\n",
              "      <td>0.918578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.082900</td>\n",
              "      <td>0.359995</td>\n",
              "      <td>0.925459</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 872\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to distilroberta-base-sst2-distilled/run-1/checkpoint-527\n",
            "Configuration saved in distilroberta-base-sst2-distilled/run-1/checkpoint-527/config.json\n",
            "Model weights saved in distilroberta-base-sst2-distilled/run-1/checkpoint-527/pytorch_model.bin\n",
            "tokenizer config file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-527/tokenizer_config.json\n",
            "Special tokens file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-527/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 872\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to distilroberta-base-sst2-distilled/run-1/checkpoint-1054\n",
            "Configuration saved in distilroberta-base-sst2-distilled/run-1/checkpoint-1054/config.json\n",
            "Model weights saved in distilroberta-base-sst2-distilled/run-1/checkpoint-1054/pytorch_model.bin\n",
            "tokenizer config file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-1054/tokenizer_config.json\n",
            "Special tokens file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-1054/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 872\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to distilroberta-base-sst2-distilled/run-1/checkpoint-1581\n",
            "Configuration saved in distilroberta-base-sst2-distilled/run-1/checkpoint-1581/config.json\n",
            "Model weights saved in distilroberta-base-sst2-distilled/run-1/checkpoint-1581/pytorch_model.bin\n",
            "tokenizer config file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-1581/tokenizer_config.json\n",
            "Special tokens file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-1581/special_tokens_map.json\n",
            "Deleting older checkpoint [distilroberta-base-sst2-distilled/run-1/checkpoint-527] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 872\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to distilroberta-base-sst2-distilled/run-1/checkpoint-2108\n",
            "Configuration saved in distilroberta-base-sst2-distilled/run-1/checkpoint-2108/config.json\n",
            "Model weights saved in distilroberta-base-sst2-distilled/run-1/checkpoint-2108/pytorch_model.bin\n",
            "tokenizer config file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-2108/tokenizer_config.json\n",
            "Special tokens file saved in distilroberta-base-sst2-distilled/run-1/checkpoint-2108/special_tokens_map.json\n",
            "Deleting older checkpoint [distilroberta-base-sst2-distilled/run-1/checkpoint-1054] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from distilroberta-base-sst2-distilled/run-1/checkpoint-2108 (score: 0.9254587155963303).\n",
            "\u001b[32m[I 2022-10-08 07:15:46,959]\u001b[0m Trial 1 finished with value: 0.9254587155963303 and parameters: {'num_train_epochs': 4, 'learning_rate': 0.00014093912322591537, 'alpha': 0.8464471686848708, 'temperature': 7}. Best is trial 1 with value: 0.9254587155963303.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BestRun(run_id='1', objective=0.9254587155963303, hyperparameters={'num_train_epochs': 4, 'learning_rate': 0.00014093912322591537, 'alpha': 0.8464471686848708, 'temperature': 7})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Updating the training arguments"
      ],
      "metadata": {
        "id": "l8X7E7FpjFg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# overwriting the previous hyperparameters\n",
        "for k,v in best_run.hyperparameters.items():\n",
        "    setattr(training_args, k, v)\n",
        "\n",
        "# new repository\n",
        "best_model_ckpt = \"distilroberta-best\"\n",
        "training_args.output_dir = best_model_ckpt"
      ],
      "metadata": {
        "id": "UqW7rvoNMil5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Training"
      ],
      "metadata": {
        "id": "qNhUWuyOjTYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# New Trainer with the updated parameters\n",
        "optimal_trainer = DistillationTrainer(\n",
        "    student_model,\n",
        "    training_args,\n",
        "    teacher_model=teacher_model,\n",
        "    train_dataset=sst2_enc[\"train\"],\n",
        "    eval_dataset=sst2_enc[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "optimal_trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v1VJCtxtMjnJ",
        "outputId": "0f2c24eb-6388-4595-a4fd-cd43c7f2bc2a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cuda_amp half precision backend\n",
            "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 67349\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 128\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2108\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2108' max='2108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2108/2108 10:16, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.144000</td>\n",
              "      <td>0.379220</td>\n",
              "      <td>0.907110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.108500</td>\n",
              "      <td>0.466671</td>\n",
              "      <td>0.911697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.078600</td>\n",
              "      <td>0.359551</td>\n",
              "      <td>0.915138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.057400</td>\n",
              "      <td>0.358214</td>\n",
              "      <td>0.920872</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 872\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to distilroberta-best/checkpoint-527\n",
            "Configuration saved in distilroberta-best/checkpoint-527/config.json\n",
            "Model weights saved in distilroberta-best/checkpoint-527/pytorch_model.bin\n",
            "tokenizer config file saved in distilroberta-best/checkpoint-527/tokenizer_config.json\n",
            "Special tokens file saved in distilroberta-best/checkpoint-527/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 872\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to distilroberta-best/checkpoint-1054\n",
            "Configuration saved in distilroberta-best/checkpoint-1054/config.json\n",
            "Model weights saved in distilroberta-best/checkpoint-1054/pytorch_model.bin\n",
            "tokenizer config file saved in distilroberta-best/checkpoint-1054/tokenizer_config.json\n",
            "Special tokens file saved in distilroberta-best/checkpoint-1054/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 872\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to distilroberta-best/checkpoint-1581\n",
            "Configuration saved in distilroberta-best/checkpoint-1581/config.json\n",
            "Model weights saved in distilroberta-best/checkpoint-1581/pytorch_model.bin\n",
            "tokenizer config file saved in distilroberta-best/checkpoint-1581/tokenizer_config.json\n",
            "Special tokens file saved in distilroberta-best/checkpoint-1581/special_tokens_map.json\n",
            "Deleting older checkpoint [distilroberta-best/checkpoint-527] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 872\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to distilroberta-best/checkpoint-2108\n",
            "Configuration saved in distilroberta-best/checkpoint-2108/config.json\n",
            "Model weights saved in distilroberta-best/checkpoint-2108/pytorch_model.bin\n",
            "tokenizer config file saved in distilroberta-best/checkpoint-2108/tokenizer_config.json\n",
            "Special tokens file saved in distilroberta-best/checkpoint-2108/special_tokens_map.json\n",
            "Deleting older checkpoint [distilroberta-best/checkpoint-1054] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from distilroberta-best/checkpoint-2108 (score: 0.9208715596330275).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2108, training_loss=0.0971325217433401, metrics={'train_runtime': 617.2345, 'train_samples_per_second': 436.457, 'train_steps_per_second': 3.415, 'total_flos': 3418702066089216.0, 'train_loss': 0.0971325217433401, 'epoch': 4.0})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}